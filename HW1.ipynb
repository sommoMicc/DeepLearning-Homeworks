{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8MTQQX3WaFa",
        "colab_type": "text"
      },
      "source": [
        "#**HW 1 : From Perceptron to DNN**\n",
        "\n",
        "In this first homework, we are going to write our own simple feedforward neural network using just Python and NumPy (the standard numeric library for Python). We will start by implementing just a simple neuron, or perceptron, then we define the training algorithm for this simple model.\n",
        "The second part consists in defining a simple neural network to perform digits classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bEFm73cYFQy",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1.1: Perceptron\n",
        "\n",
        "In this first exercise, we will implement a simple neuron, or perceptron, as visualized below. We will have just three inputs and one output neuron (we will skip the bias for now).\n",
        "Notice how the perceptron just performs a sum of the individual inputs multiplied by the corresponding weights mapped through an activation function $f(\\cdot)$.  This can also be expressed as a dot product of the weight vector $\\textbf{W}$ and the input vector $\\textbf{x}$. Thus: $\\hat{y}=f(\\textbf{W}^T \\textbf{x})$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDlidWmiYuKB",
        "colab_type": "text"
      },
      "source": [
        "In this first part we will implement the perpetron by using [numpy](https://docs.scipy.org/doc/numpy/reference/) library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0UCur_TYckH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQONq1k6Y1Zx",
        "colab_type": "text"
      },
      "source": [
        "### Training data\n",
        "\n",
        "Let's consider a very simple dataset. The dataset is composed of the inputs value $x \\in \\mathbb{R}^3$ and the desired target values. Below, each row is a single example: the first three columns the input and the last column the target output.\n",
        "\n",
        "    0 0 1  0  \n",
        "    0 1 1  0  \n",
        "    1 0 1  1  \n",
        "    1 1 1  1  \n",
        "\n",
        "Note that our target outputs are equal to the first column of the input. Therefore the task that the model should learn is very simple. We will see if it can learn that just from the data.\n",
        "\n",
        "Now let's define the `X` and `y` matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1FzAUxhY9PA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our input data is a matrix, each row is one input sample\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "    \n",
        "# The target output as a column vector in 2-D array format (.T means transpose)\n",
        "y = np.array([[0,0,1,1]]).T\n",
        "\n",
        "print('X=',X)\n",
        "print('y=',y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-W8wMnoZH71",
        "colab_type": "text"
      },
      "source": [
        "### Activation function\n",
        "\n",
        "As we said before, in order to define a perceptro we need to define the activation function $f(\\cdot)$. There are many possibile activation function that can be use. Let's plot some commonly used activation functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi5ZK9zKZssu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "x = np.arange(-4,4,.01)\n",
        "plt.figure()\n",
        "plt.plot(x, np.maximum(x,0), label='ReLu')\n",
        "plt.plot(x, 1/(1+np.exp(-x)), label='sigmoid')\n",
        "plt.plot(x, np.tanh(x), label='tanh')\n",
        "plt.axis([-4, 4, -1.1, 1.1])\n",
        "plt.title('Activation functions')\n",
        "l = plt.legend()\n",
        "\n",
        "# Delete temporary variables, so not to cause any confusion later :-)\n",
        "del x, l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmJ6-qaZjuZ",
        "colab_type": "text"
      },
      "source": [
        "In this particular case we will use the sigmoid function. So, let's define $f(\\cdot)$ as the sigmoid function\n",
        "\n",
        "$\\sigma(x)=\\frac{1}{1+\\exp^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwnk5RgKZRox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX-62n3Io0XU",
        "colab_type": "text"
      },
      "source": [
        "### Weight initialization\n",
        "\n",
        "We have to initialise our weights. Let's initialize them randomly, so that their mean is zero. The weights matrix map the input space into the output space, therefore in our case $\\mathbf{W} \\in \\mathbb{R}^{3 \\times 1}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f83YkaNtd5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed([42])\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "W = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "print('W=', W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZd-aQxAuX49",
        "colab_type": "text"
      },
      "source": [
        "### Forward propagation\n",
        "\n",
        "Now let's try one round of forward propagation.  This means taking an input sample and moving it forward through the network, finally calculating the output of the network.\n",
        "\n",
        "For our single neuron this is simply $\\hat{\\mathbf{y}} = f(\\mathbf{W}^T \\mathbf{x})$, where $\\mathbf{x}$ is one input vector.\n",
        "\n",
        "each input sample is arranged as a row of the matrix `X`, therefore we can access the first row by `X[0]`. Let's store it in the variable `X0` for easier access. We'll use `reshape` to make sure it's expressed as a column vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gTx4QxUumvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X0=np.reshape(X[0], (3,1))\n",
        "print(X0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpDCnmxWurjC",
        "colab_type": "text"
      },
      "source": [
        "The output $\\hat{y}$ for the first input can be calculated according to the formula given above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idDZfTVKuyPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = f(np.dot(W.T, X0))\n",
        "\n",
        "print('y_out=', y_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oda3Y3btu8Uz",
        "colab_type": "text"
      },
      "source": [
        "the target result is stored in `y[0]`.  If you check back, you can see we defined it to be 0. You can see that our network is pretty far away from the right answer. This is why we need to backpropagate the error, to adjust the weights in the right direction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGvgZCDFu-EM",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation\n",
        "\n",
        "The next step is to update the weights by propagating the error backward in the network.  Exactly how this is done depends on the activation function, or more specifically its derivative. The activation function of the considered model is the sigmoid, its derivative is:\n",
        "\n",
        "$\\sigma(x) \\cdot (1-\\sigma(x))$\n",
        "\n",
        "Recall that the weight update is given as $\\Delta w_{ji} = -\\epsilon \\delta_j x_i$. Our network has only one layer, so $x_i$ is just the input, i.e., and a single output neuron so there is no need for index $j$. \n",
        "\n",
        "In matrix form we can calculate this for all the weights:\n",
        "\n",
        "$$\\Delta \\textbf{W} = -\\epsilon \\delta \\textbf{x}_0$$\n",
        "where $\\delta$ is the gradient (grad in the code) and $\\textbf{x}_0$ is our first input sample in variable `X0`.\n",
        "\n",
        "Recall that $y$ is the desired output, i.e. `y[0]` in the Python code, and $\\hat{y}$ is `y_out` here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs0EG3n9ve6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the learning rate determines the step size in the gradient descent, you can experiment with different values if you want\n",
        "learning_rate = 0.5 \n",
        "\n",
        "# compute the gradient term\n",
        "grad = (y_out - y[0])*y_out*(1 - y_out)\n",
        "\n",
        "# Calculate the weight update\n",
        "W_delta = -learning_rate * grad * X0\n",
        "\n",
        "print(W_delta)\n",
        "\n",
        "# Update the weights\n",
        "W += W_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha1gJ0n9v2oV",
        "colab_type": "text"
      },
      "source": [
        "Let's try a forward propagation again with the same input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh40oHl2v6rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out=f(np.dot(W.T, X0))\n",
        "\n",
        "print('y_out=', y_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHc3os_AL54y",
        "colab_type": "text"
      },
      "source": [
        "You should notice that the result has moved (very slightly) towards the correct answer (that is zero). In order to converge to the right value we have to perform more iterations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INLy50fyORN",
        "colab_type": "text"
      },
      "source": [
        "### Training iterations\n",
        "\n",
        "Let's define a complete training procedure for our model. In each iteration we have to perform the forward propagation, then we'll check how much the output differs from the target and propagate the error back (backward propagation).  We'll do this for each sample data point and then iterate this over and over again using a for loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPXJwQm4ycgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the training we need to iterate over the data set several times\n",
        "num_iters = 1000\n",
        "\n",
        "learning_rate = 0.5\n",
        "\n",
        "# We'll also calculate the mean square error (MSE) in every round so we can see how it develops,\n",
        "# mse is just an array to store these for each round\n",
        "mse = np.zeros(num_iters)\n",
        "\n",
        "# For-loop for the iterations\n",
        "for it in range(num_iters):\n",
        "    \n",
        "    # For-loop going over each sample in X\n",
        "    for n in range(len(X)):\n",
        "        # Extract the n_th sample and the corresponding desired output\n",
        "        x_n = np.reshape(X[n], (3,1))\n",
        "        # Get the correponding target value\n",
        "        y_target = y[n]\n",
        "        \n",
        "        # Forward propagation\n",
        "        y_out = f(np.dot(W.T, x_n))\n",
        "\n",
        "        # Let's keep track of the sum of squared errors\n",
        "        mse[it] += #To Complete: compute mean squared error between y_target and y_out\n",
        "    \n",
        "        # compute the gradient\n",
        "        grad = (y_out - y_target)*y_out*(1 - y_out) \n",
        "    \n",
        "        # Calculate the weights update\n",
        "        W_delta = -learning_rate * grad * x_n\n",
        "\n",
        "        # Update the weights\n",
        "        W += W_delta\n",
        "\n",
        "    # Divide by the number of elements to get the mean of the squared errors\n",
        "    mse[it] /= len(X)\n",
        "\n",
        "\n",
        "y_out = f(np.dot(X, W))\n",
        "print(\"Output after training, y_out\")\n",
        "print(y_out)\n",
        "print(\"Target output, y\")\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT1qLBvBzpDc",
        "colab_type": "text"
      },
      "source": [
        "After the training phase, the output of the network is pretty close to the target output. \n",
        "\n",
        "How many iterations were required in order to obtain this result? We have set the number of the iteration to 1000, but it is interesting to study the trend of the error trought the iterations. In the next homework, we will analyze how to select the right number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6RsKlA7zrw3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(range(num_iters), mse, label=\"MSE\")\n",
        "l = plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiS6hr-eMWR1",
        "colab_type": "text"
      },
      "source": [
        "You should see the error going down pretty quickly in the beginning and then slowing down."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zoz4y02rxorc",
        "colab_type": "text"
      },
      "source": [
        "### Batch training\n",
        "\n",
        "With real-world data it is very slow to handle each example one-by-one like we did above.  Instead one typically uses so called mini batches of several input examples at once.\n",
        "\n",
        "Lets consider a subset of samples from the training set. Each of these samples is one row in $\\textbf{X}$, instead of a single column vector as before. The forward propagation step looks a bit different mathematically: $\\hat{\\textbf{y}} = f(\\textbf{X}\\textbf{W})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwc6hfIlxspG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out = f(np.dot(X, W))\n",
        "print(y_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8pBN8voyLGv",
        "colab_type": "text"
      },
      "source": [
        "Then we will get the corresponding output (each value in `y_out`) for each input (each row in `X`) in single matrix multiplication.  The error and weight updates can similarly all be calculated in a single go using matrix multiplications similarly to the steps we did above with single vectors.\n",
        "\n",
        "In these exercises, we'll stick to just doing one sample at a time, as the batch mode makes it a bit more complicated to understand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv8Vplo1F7E",
        "colab_type": "text"
      },
      "source": [
        "## **Exercise 1.2. Two layer network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiI7bIaY1OVI",
        "colab_type": "text"
      },
      "source": [
        "Now let's try a slightly more difficult example. We'll use the same input data, but a different desired output. \n",
        "\n",
        "    0 0 1  0  \n",
        "    0 1 1  1  \n",
        "    1 0 1  1  \n",
        "    1 1 1  0 \n",
        "\n",
        "In particular, the new input-output configuration represents the XOR problem (the last column of the input data is just ones, and thus irrelevant). This problem is interesting because it can not be solved by using a single layer perceptron. Indeed, you will need (at least) a two-layer network to solve it.\n",
        "In this exercise we will first show that the network that we defined in the previous exercise, can not solve the XOR problem,  then, we will define a 2 layer perceptron able to compute the correct solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuSfKOpD1TqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "np.random.seed([42])\n",
        "\n",
        "X = np.array([[0,0,1],\n",
        "              [0,1,1],\n",
        "              [1,0,1],\n",
        "              [1,1,1]])\n",
        "    \n",
        "y = np.array([[0,1,1,0]]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FphvrH-1e5O",
        "colab_type": "text"
      },
      "source": [
        "As we did in the previous exercise let's initialize the weights and define the activation function, also in this exercise we will use the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMeTD8cb1kNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Weights initialization\n",
        "W = 2*np.random.random((3,1)) - 1\n",
        "\n",
        "# Activation function\n",
        "def f(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unbPv9pn3kVE",
        "colab_type": "text"
      },
      "source": [
        "Now, let's run the network previously defined to check if it is able to solve the XOR problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGlUPnvi31fL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iters = 1000\n",
        "learning_rate = 0.5\n",
        "\n",
        "for it in range(num_iters):\n",
        "    for n in range(len(X)):\n",
        "        x_n = np.reshape(X[n], (3,1))\n",
        "        y_target = y[n]\n",
        "        \n",
        "        # Forward propagation\n",
        "        y_out = f(np.dot(W.T, x_n))\n",
        "\n",
        "        # Compute the Gradient\n",
        "        grad = (y_out - y_target)*y_out*(1 - y_out)\n",
        "    \n",
        "        # Calculate the weights update\n",
        "        W_delta = -learning_rate * grad * x_n\n",
        "\n",
        "        # Update the weights\n",
        "        W += W_delta\n",
        "\n",
        "\n",
        "# Now let's see the output for each input sample with the trained weights\n",
        "# Using batch mode we can do this in a single line\n",
        "y_out = f(np.dot(X, W))\n",
        "print(\"Output after training, y_out\")\n",
        "print(y_out)\n",
        "print(\"Desired output, y\")\n",
        "print(y)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myYbripS39R-",
        "colab_type": "text"
      },
      "source": [
        "As you can see the network is not able to solve the problem, it's not even close. You can try to increase the number of iterations, but it won't help.\n",
        "Let's add a single hidden layer, for example with 4 hidden nodes (you can experiment with this number).\n",
        "The input to the network is $\\mathbf{x}$ as before.  The first hidden layer calculates $\\textbf{h} = f(\\textbf{W}_1^Tx)$, note that $\\textbf{W}_1$ is now $\\in \\mathbb{R}^{3 \\times 4}$.  The output layer calculates $\\hat{y} = f(\\textbf{W}_2^T\\textbf{h})$, where ${W}_2 \\in \\mathbb{R}^{4 \\times 1}$.\n",
        "\n",
        "We'll start by initializing the weights randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llke92kx4MRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_hidden = 4\n",
        "\n",
        "# initialize weights randomly with mean 0\n",
        "W_1 = 2*np.random.random((3,num_hidden)) - 1\n",
        "W_2 = 2*np.random.random((num_hidden,1)) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnQfHhXV7dO0",
        "colab_type": "text"
      },
      "source": [
        "We have to define the training procedure in order to manage the two layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slOBh8IJ4tSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iters = 2000\n",
        "eps = 0.5\n",
        "\n",
        "mse = np.zeros(num_iters)\n",
        "\n",
        "for it in range(num_iters):\n",
        "    for n in range(len(X)):\n",
        "        x_n = np.reshape(X[n], (3,1))\n",
        "        y_target = y[n]        \n",
        "        \n",
        "        # Forward propagation\n",
        "        # Calculate h\n",
        "        h = f(np.dot(W_1.T, x_n))\n",
        "    \n",
        "        # Calculate y_out\n",
        "        y_out = f(np.dot(W_2.T, h))\n",
        "        \n",
        "        # Let's keep track of the sum of squared errors\n",
        "        mse[it] += #To Complete: compute mean squared error between y_target and y_out\n",
        "\n",
        "        # To complete: Compute the gradient\n",
        "        \n",
        "        # hint: you can do this by performing a for loop over i (hidden nodes) and k (input nodes) and calculate \n",
        "        # each W_1_ik update separately\n",
        "       \n",
        "        # To complete: Update the weights\n",
        "        # Note: it's important the W weights are updated at the end,\n",
        "        # the above calculation should be done with the old weights\n",
        "        \n",
        "\n",
        "    # Divide by the number of elements to get the mean of the squared errors\n",
        "    mse[it] /= len(X)\n",
        "\n",
        "y_out = f(np.dot(f(np.dot(X, W_1)), W_2))\n",
        "print(\"Output after training, y_out\")\n",
        "print(y_out)\n",
        "print(\"Target output, y\")\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nYoyoR2fANg",
        "colab_type": "text"
      },
      "source": [
        "Now you should see outputs very similar to the desired ones "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YKuG4wee9kIc",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(range(num_iters), mse, label=\"MSE\")\n",
        "l = plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAmf-t0PurMP",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1.3: handwritten digits classification\n",
        "In this exercise, we try to apply what we learned in the previous exercise in a real-world scenario. In particular, we consider a simple digits classification problem. The model turns out to be similar to the perceptron implemented in Exercise 1.1, but here we will use softmax activation function and cross-entropy loss function. The idea is to create a model that has in input an image of a handwritten digit and that return a vector of 10 probabilities (one for each possible digit 0 - 9). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAYb_cd8vkG-",
        "colab_type": "text"
      },
      "source": [
        "### Dataset:\n",
        "The dataset that we will use in this exercise is contained in [scikit-learn](https://scikit-learn.org/stable/). The dataset contains several samples. Each sample is composed of the image of the handwritten digit, a numeric representation of the image  (that will be the input of our model) and the target (the digit itself). \n",
        "\n",
        "Let's start by plotting one of this handwritten digit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrkAeWtufMk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "np.random.seed([42])\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "sample_index = 45\n",
        "plt.figure(figsize=(3, 3))\n",
        "plt.imshow(digits.images[sample_index], cmap=plt.cm.gray_r,\n",
        "           interpolation='nearest')\n",
        "plt.title(\"image label: %d\" % digits.target[sample_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0OsnNJbv69J",
        "colab_type": "text"
      },
      "source": [
        "Check how an input $\\mathbf{X}$ and its related target $\\mathbf{y}$ are represented in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei9Uda4av5q2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.asarray(digits.data, dtype='float32')\n",
        "target = np.asarray(digits.target, dtype='int32')\n",
        "\n",
        "print(\"X:\",data)\n",
        "\n",
        "print(\"y:\",target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hc97pziwLKa",
        "colab_type": "text"
      },
      "source": [
        "#### One-hot encoding\n",
        "In order to have a representation of the target that will be similar to the output of the model, we will use one-hot encoding. Basically, the one-hot encoding allows to encode a categorical integer feature using a one-of-K scheme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eF6jEnOmwRUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(n_classes, y):\n",
        "    return np.eye(n_classes)[y]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4CISmJ39lts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot(n_classes=10, y=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkSruP-bwkki",
        "colab_type": "text"
      },
      "source": [
        "### Activation Function: Softmax\n",
        "As activation function we will use the Softmax function: This particular function is very useful when we have to deal with classification tasks and one-hot target because it turns numbers aka logits (pre-activations) into probabilities that sum to one. Basically, Softmax function outputs a vector that represents the probability distributions of a list of potential outcomes.\n",
        "$$\n",
        "softmax(\\mathbf{x}) = \\frac{e^{h_i}}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
        "$$\n",
        "In our case we have in input a matrix $\\mathbf{X}$ where each row is a vector $\\mathbf{x}$, therefore the softmax function that we have to implements will be mathematically defined as:\n",
        "$$\n",
        "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "  e^{x_1}\\\\\\\\\n",
        "  e^{x_2}\\\\\\\\\n",
        "  \\vdots\\\\\\\\\n",
        "  e^{x_n}\n",
        "\\end{bmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BuHFgAKws_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    #To complete: define softmax function\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KP7xOmNqsE-R",
        "colab_type": "text"
      },
      "source": [
        "### Loss Function: Cross Entropy ###\n",
        "Usualy, a neural network-based classifier that use the softmax function in the final layer is commonly trained using Cross-Entropy as loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYnPJ1lWsFkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPSILON = 1e-8\n",
        "\n",
        "def cross_entropy(Y_true, Y_pred):\n",
        "\n",
        "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
        "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
        "    return -np.mean(loglikelihoods)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2jR9ijztQWP",
        "colab_type": "text"
      },
      "source": [
        "### Weights Initailiazation\n",
        "\n",
        "Similarly to what we did in previous exercises, we have to initialize the weights, but in this case, we will consider also the bias term. Therefore we have the weights $\\mathbf{W}\\in\\mathbb{R}^{m \\times n}$ and the bias $\\mathbf{b}\\in\\mathbb{R}^m$, where $n$ is the input size, and $m$ the number of classes.\n",
        "Now we can define the output of our model\n",
        "\n",
        "$\\hat{\\mathbf{y}}=softmax(\\textbf{W} \\textbf{x}+\\mathbf{b})$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtgOtaVL-4xd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size=data.shape[1]\n",
        "n_classes = len(np.unique(target))\n",
        "\n",
        "\n",
        "W = np.random.uniform(size=(input_size,n_classes),high=0.1, low=-0.1)\n",
        "\n",
        "b = np.random.uniform(size=n_classes, high=0.1, low=-0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ab3_mFz-9kG",
        "colab_type": "text"
      },
      "source": [
        "Let's consider a sample from the training set, and plot the current output of our model, before training it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VpIr1KY_CTd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_out=softmax(np.dot(data[sample_index], W) + b)\n",
        "\n",
        "plt.bar(range(n_classes),y_out,label='prediction', color=\"red\")\n",
        "plt.ylim(0,1,0.1)\n",
        "plt.xticks(range(n_classes))\n",
        "plt.legend()\n",
        "plt.ylabel(\"probability\")\n",
        "plt.title(\"target:\"+str(target[sample_index]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4DYTEEh_QDf",
        "colab_type": "text"
      },
      "source": [
        "### Training procedure:\n",
        "As in the previous exercise let's define a training procedure. Note that in this case, we have to compute the gradient according to the softmax function and the loss function that the training has to optimize. \n",
        "\n",
        "In this case, the gradient for the weights W is:\n",
        "\n",
        "$\\nabla_W=(\\mathbf{\\hat{y}}-\\mathbf{y}) \\cdot \\mathbf{x}$\n",
        "\n",
        "while for the bias is:\n",
        "\n",
        "$\\nabla_b=(\\mathbf{\\hat{y}}-\\mathbf{y})$\n",
        "\n",
        "During the training procedure let's compute the accuracy of the predictions and the loss value at each iteration:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcQfByoE_a6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_iters = 50\n",
        "learning_rate = 0.0005\n",
        "\n",
        "for it in range(num_iters):\n",
        "    iteration_accuracy=[]\n",
        "    iteration_loss=[]\n",
        "    for i, (X, y) in enumerate(zip(data, target)):\n",
        "\n",
        "        # Forward propagation\n",
        "        y_out=softmax(np.dot(X, W) + b)\n",
        "\n",
        "        #Eval the output error\n",
        "        pred_err = y_out - one_hot(n_classes, y)\n",
        "\n",
        "\n",
        "        # To complete: Compute the gradient (for the weights and the bias)\n",
        "\n",
        "        # To complete: Update the weights and the bias\n",
        "        # Note: it's important the weights W and the bias b are updated at the end\n",
        "        # the above calculation should be done with the old weights\n",
        "\n",
        "        iteration_accuracy.append(np.argmax(y_out) == y)\n",
        "        iteration_loss.append(cross_entropy(one_hot(n_classes,y),y_out))\n",
        "\n",
        "    print(\"iteration: \",it,\" -- accuracy: \",np.mean(np.asarray(iteration_accuracy)), \" -- loss: \", np.mean(iteration_loss))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnJH8lWaxtc4",
        "colab_type": "text"
      },
      "source": [
        "As you can see during the training the accuracy increase after each iteration, while the loss function value progressively decreases.\n",
        "\n",
        "Let's now check how the prediction capability of our model change after the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGrzV9w6_lwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=softmax(np.dot(data[sample_index], W) + b)\n",
        "plt.bar(range(n_classes), y_pred, label='prediction', color=\"red\")\n",
        "plt.ylim(0, 1, 0.1)\n",
        "plt.xticks(range(n_classes))\n",
        "plt.legend()\n",
        "plt.ylabel(\"probability\")\n",
        "plt.title(\"target:\"+str(target[sample_index]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}